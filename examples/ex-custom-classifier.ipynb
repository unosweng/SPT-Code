{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets are loaded. Total # of tweets: 7613.\n",
      "# of labels:\n",
      "target\n",
      "0    4342\n",
      "1    3271\n",
      "Name: count, dtype: int64\n",
      "The number of tweeets which appear multiple times: 19\n",
      "Tweets which have inconsistent labeling:\n",
      "\n",
      "To fight bioterrorism sir.\n",
      "[1, 0, 1, 0]\n",
      ".POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4\n",
      "[1, 1, 0, 1]\n",
      "He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\n",
      "[0, 1, 1, 0, 0, 0]\n",
      "Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\n",
      " \n",
      "#FARRAKHAN #QUOTE\n",
      "[1, 0, 0]\n",
      "#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption\n",
      "[1, 1, 0]\n",
      "The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\n",
      "[0, 0, 1, 0, 0, 1]\n",
      "Hellfire is surrounded by desires so be careful and donÛªt let your desires control you! #Afterlife\n",
      "[0, 1, 0]\n",
      "#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\n",
      "[0, 0, 1]\n",
      "CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring\n",
      "[1, 1, 0]\n",
      "that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\n",
      "[1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Hugging Face BERT with custom classifier (PyTorch)\n",
    "\n",
    "https://www.kaggle.com/code/angyalfold/hugging-face-bert-with-custom-classifier-pytorch/notebook\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_csv_path = './train.csv'\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "\n",
    "all_texts = train_df['text'].values.tolist()\n",
    "all_labels = train_df['target'].values.tolist()\n",
    "\n",
    "print(\"Tweets are loaded. Total # of tweets: {}.\".format(len(all_texts)))\n",
    "print(\"# of labels:\")\n",
    "print(train_df['target'].value_counts())\n",
    "\n",
    "# As it turns out there are couple of tweets which occurs multiple times. Among those there are some whose labels aren't consistent throughout the occurrences.\n",
    "\n",
    "frequent_tweets = {}\n",
    "for t, l in zip(all_texts, all_labels):\n",
    "    if all_texts.count(t) > 2:\n",
    "        frequent_tweets[t] = [l] if t not in frequent_tweets else frequent_tweets[t] + [l]\n",
    "        \n",
    "print(\"The number of tweeets which appear multiple times: {}\"\n",
    "      .format(len(frequent_tweets.keys())))     \n",
    "\n",
    "print(\"Tweets which have inconsistent labeling:\")\n",
    "print()\n",
    "\n",
    "for t, ls in frequent_tweets.items():\n",
    "    if not all(element == ls[0] for element in ls):\n",
    "        print(t)\n",
    "        print(ls)\n",
    "\n",
    "# The amount of tweets with inconsistent labeling seems reasonably low so they can be fixed by hand. (Note: One could argue that deleting tweets with inconsistent labeling would be a better practice because modifing the input like that is an overreach, but for the sake of the example I go with releballing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relabeled 10 tweets in total\n"
     ]
    }
   ],
   "source": [
    "should_be_real = [\".POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4\",\n",
    "                 \"#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption\",\n",
    "                 \"CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring\"]\n",
    "\n",
    "should_not_be_real = [\"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\",\n",
    "                     \"Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\",\n",
    "                      \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\",\n",
    "                     \"Hellfire is surrounded by desires so be careful and donÛªt let your desires control you! #Afterlife\",\n",
    "                     \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\",\n",
    "                     \"that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\",\n",
    "                     \"To fight bioterrorism sir.\"]\n",
    "\n",
    "\n",
    "def fix_labels(tweets_to_fix, correct_label):\n",
    "    for i, (tweet, label) in enumerate(zip(all_texts, all_labels)):\n",
    "        if any(tweet.startswith(t) for t in tweets_to_fix):\n",
    "            all_labels[i] = correct_label\n",
    "\n",
    "        \n",
    "fix_labels(should_be_real, 1)\n",
    "fix_labels(should_not_be_real, 0)\n",
    "\n",
    "print(\"Relabeled {} tweets in total\".format(len(should_be_real) + len(should_not_be_real)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data is read and split into training and validation sets.\n",
      "Size of train data (# of entries): 5709\n",
      "Size of validation data (# of entries): 1904\n"
     ]
    }
   ],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    all_texts, all_labels,\n",
    "    stratify = train_df['target']\n",
    ")\n",
    "\n",
    "print('Train data is read and split into training and validation sets.')\n",
    "print('Size of train data (# of entries): {}'.format(len(train_texts)))\n",
    "print('Size of validation data (# of entries): {}'.format(len(val_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tweets cleaned.\n",
      "Validation tweets cleaned.\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning\n",
    "\n",
    "\"\"\"\n",
    "The most obvious step to take is to remove URLs as they are most likely just noise.\n",
    "\n",
    "An additional consideration to take into account is that Hugging Face's tokenizer employs subword tokenization as detailed in their summary here. It essentialy means that if the tokenizer encounters a word which is unknown to it the word gets splitted into multiple tokens. Each new token gets the '##' prefix. For example: \"annoyingly\" becomes \"annoying\" + \"##ly\". Now it is easy to figure out which words are unknown to the model (just by searching for the '##' prefix) and thus gain ideas what sort of cleaning might worth implementing.\n",
    "\n",
    "In this implementation URLs, @ links, non ascii characters are completely removed, the negation of some of the auxiliary verbs are fixed (eg.: shouldnt -> should not) and some of the personal pronouns (eg.: im -> i am)\n",
    "\"\"\"\n",
    "\n",
    "from ex_custom_classifier_helper import clean_tweet \n",
    "\n",
    "\n",
    "cleaned_train_texts = [clean_tweet(tweet) for tweet in train_texts]\n",
    "print(\"Train tweets cleaned.\")\n",
    "cleaned_val_texts = [clean_tweet(tweet) for tweet in val_texts]\n",
    "print(\"Validation tweets cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train & validation texts encoded\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# The tokenizer's truncation=True setting ensures that the sequence of tokens is truncated if the sequence is longer than the maximal input length acceptable by the model. padding=True ensures that each sentence is padded to the longest sentence of the batch.\n",
    "train_encodings = tokenizer(cleaned_train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(cleaned_val_texts, truncation=True, padding=True)\n",
    "print('Train & validation texts encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Class to store the tweet data as PyTorch Dataset\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Custom dataset\n",
    "\n",
    "\"\"\"\n",
    "PyTorch uses datasets and dataloaders to handle data (see their introductionary tutorial here https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). It means that in order to make the handling of tweets straightforward a custom dataset has to be defined. (Named TweetDataset in this code)\n",
    "\n",
    "A dataset is a data structure which makes it easy to iterate through the data in training and testing loops, therefore it needs to implement three methods of its base class (which is torch.utils.data.Dataset): __init__ (to initialize the dataset with the data), __len__ (to get the number of items in the dataset) and __getitem__ (to return the ith element of the dataset).\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Class to store the tweet data as PyTorch Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # an encoding can have keys such as input_ids and attention_mask\n",
    "        # item is a dictionary which has the same keys as the encoding has\n",
    "        # and the values are the idxth value of the corresponding key (in PyTorch's tensor format)\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "print(TweetDataset.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom model\n",
    "\n",
    "\"\"\"\n",
    "A pre-trained BERT model with custom classifier. Based on this notebook.\n",
    "\n",
    "The custom model consists of a pre-trained BERT model (a model which holds a semantical representation of English) and on the top of the BERT model there is a custom neural network which is trained to the specific task (tweet classification in this case). Therefore, it seems to be reasonable to have freeze_bert and unfreeze_bert methods apart from the mandatory __init__ and forward. Having this two additional methods makes it possible to sort of train the underlying BERT model and the custom classifier separately. (So train BERT and the custom head together, freeze BERT and then train the custom head on the classification task based on the previously trained BERT). The idea of freezing & unfreezing was taken from Milan Kalkenings' notebook.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spt-code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
